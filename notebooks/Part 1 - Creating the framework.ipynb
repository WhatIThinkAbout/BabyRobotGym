{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[A Baby Robot's Guide To Reinforcement Learning](https://towardsdatascience.com/tagged/baby-robot-guide)__\n",
    "\n",
    "# Creating a Custom Gym Environment for Jupyter Notebooks\n",
    "## Part 1: Creating the framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/drawgrid.gif\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "This notebook accompanies the Towards Data Science article and is part of _[A Baby Robot's Guide To Reinforcement Learning](https://towardsdatascience.com/tagged/baby-robot-guide)_\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This article (split over two parts) describes the creation of a custom _[OpenAI Gym](https://www.gymlibrary.ml/)_ environment for *Reinforcement Learning* (_RL_) problems. \n",
    "\n",
    "Quite a few tutorials already exist that show how to create a custom Gym environment (see the _[References]()_ section for a few good links). In all of these examples, and indeed in the most common Gym environments, these produce either a text-based output (e.g. _[Frozenlake](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/)_) or an image-based output (e.g. _[Lunar Lander](https://www.gymlibrary.ml/environments/box2d/lunar_lander/)_) that appears in a separate graphical window.\n",
    "\n",
    "Instead we'll create a custom environment that is specifically tailored to generate its output in a _Jupyter_ notebook. The graphical representation of the environment will be written directly into the notebook cell and updated in real time. Additionally, it can be used in any test framework, and with any RL algorithm, that also implements the _Gym_ interface.\n",
    "\n",
    "By the end of the article we will have created a custom _Gym_ environment, that can be tailored to produce a range of different Grid Worlds for _Baby Robot_ to explore, and that renders an output similar to the cover image shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/green_babyrobot_small.gif\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Up until now, in our _[series on Reinforcement Learning](https://towardsdatascience.com/tagged/baby-robot-guide)_ (RL), we've used bespoke environments to represent the locations where Baby Robot finds himself. Starting from a simple grid world we added components, such as walls and puddles, to increase the complexities of the challenges that _Baby Robot_ faced. \n",
    "\n",
    "Now that we know the basics of _RL_, and before we move onto more complex problems and algorithms, it seems like a good time to formalise _Baby Robot's_ environment. If we give this environment a fixed, defined, interface then we can re-use the same environment in all of our problems and with multiple _RL_ algorithms. This will makes things a lot simpler as we move forwards to look at different _RL_ methods.\n",
    "\n",
    "By adopting a common interface we can then drop this environment into any existing systems that also implement the same interface. All we need to do is decide what interface we should use. Luckily for us this has already been done, and it's called the <b>OpenAI Gym</b> interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to OpenAI Gym\n",
    "\n",
    "_[OpenAI Gym](https://www.gymlibrary.ml/)_ is a set of _Reinforcement Learning (RL)_ environments, with problems ranging from simple grid worlds up to complex physics engines. \n",
    "Each of these environments implements the same interface, making it easy to test a single environment using a range of different _RL_ algorithms. Similarly, it makes it straightforward to evaluate a single _RL_ algorithm on a range of different environments. \n",
    "\n",
    "As a result, _OpenAI Gym_ has become the _de-facto_ standard for learning about and bench-marking _RL_ algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The OpenAI Gym Interface\n",
    "\n",
    "The interface for all _OpenAI Gym_ environments can be divided into 3 parts:\n",
    "\n",
    "1. <b><i>Initialisation</i></b>: Create and initialise the environment.\n",
    "\n",
    "2. <b><i>Execution</i></b>: Take repeated actions in the environment. At each step the environment provides information to describe its new state and the reward received as a consequence of taking the specified action. This continues until the environment signals that the episode is complete.\n",
    "\n",
    "3. <b><i>Termination</i></b>: Cleanup and destroy the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: The CartPole Environment\n",
    "\n",
    "One of the simpler problems in Gym is the _[CartPole](https://gym.openai.com/envs/CartPole-v1/)_ environment. In this problem the goal is to move a cart left or right so that the pole, that's balanced on the cart, remains upright. \n",
    "\n",
    "<br/>\n",
    "<center><img src=\"./images/small_cartpole.gif\"/></center>\n",
    "<center><i>Figure 1: Output of the CartPole Environment - the aim is to balance the pole by moving the cart left or right.</i></center><br/>\n",
    "\n",
    "\n",
    "The code to set up and run this Gym environment is shown below. Here we're just choosing left or right actions randomly, so the pole isn't going to stay up for very long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "#         Stage 1 - Initialization\n",
    "###########################################\n",
    "\n",
    "# create the cartpole environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# run for 10 episodes\n",
    "for episode in range(10):\n",
    "\n",
    "  # put the environment into its start state\n",
    "  env.reset()\n",
    "\n",
    "###########################################\n",
    "#            Stage 2 - Execution\n",
    "###########################################\n",
    "\n",
    "  # run until the episode completes\n",
    "  done = False\n",
    "  while not done:\n",
    "\n",
    "    # show the environment\n",
    "    env.render()\n",
    "\n",
    "    # choose a random action\n",
    "    action = env.action_space.sample()   \n",
    "\n",
    "    # take the action and get the information from the environment\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "  \n",
    "  \n",
    "###########################################\n",
    "#           Stage 3 - Termination\n",
    "###########################################\n",
    "  \n",
    "# terminate the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code given above, we've labelled the 3 stages of a Gym environment. In more detail, each of these do the following:\n",
    "\n",
    "### 1. Initialisation\n",
    "\n",
    "```Python\n",
    "env = gym.make('CartPole-v0')\n",
    "```\n",
    "\n",
    "* Create the required environment, in this case the version 0 of CartPole. The returned environment object 'env' can then be used to call the functions in the common Gym environment interface.\n",
    "<br/>\n",
    "\n",
    "```Python\n",
    "obs = env.reset()\n",
    "```\n",
    "\n",
    "* Called at the start of each episode, this puts the environment into its starting state and returns the initial observation of the environment.\n",
    "<br/><br/><br/>\n",
    "\n",
    "\n",
    "### 2. Execution\n",
    "\n",
    "Here we run until the environment 'done' flag is set to indicate that the episode is complete. This can occur if the agent has reached the termination state or a fixed number of steps have been executed.\n",
    "\n",
    "```Python\n",
    "env.render()\n",
    "```\n",
    "\n",
    "* Draw the current state of the environment. In the case of CartPole this will result in a new window being opened to display a graphical view of the cart and its pole. In simpler environments, such as the FrozenLake simple grid world, a textual representation is shown.\n",
    "<br/>\n",
    "\n",
    "\n",
    "```Python\n",
    "action = env.action_space.sample()\n",
    "```\n",
    "\n",
    "* Choose a random action from the environment's set of possible actions.\n",
    "<br/>\n",
    "\n",
    "```Python\n",
    "obs, reward, done, info = env.step(action)\n",
    "```\n",
    "\n",
    "* Take the action and get back information from the environment about the outcome of this action. This includes 4 pieces of information:\n",
    "<br/><br/><br/>\n",
    "**'obs'**: Defines the new state of the environment. In the case of CartPole this is information about the position and velocity of the pole. In a grid-world environment it would be information about the next state, where we end up after taking the action.\n",
    "<br/><br/>\n",
    "**'reward'**: The amount of reward, if any, received as a result of taking the action.\n",
    "<br/><br/>\n",
    "**'done'**: A flag to indicate if we've reached the end of the episode\n",
    "<br/><br/>\n",
    "**'info'**: Any additional information. In general this isn't set.\n",
    "<br/><br/><br/>\n",
    "\n",
    "\n",
    "### 3. Termination\n",
    "\n",
    "```Python\n",
    "env.close()\n",
    "```\n",
    "\n",
    "* Terminate the environment. This will also close any graphical window that may have been created by the render function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Creating a Custom Gym Environment\n",
    "\n",
    "As described previously, the major advantage of using <i>OpenAI Gym</i> is that every environment uses exactly the same interface. We can just replace the environment name string '<i>CartPole-v0</i>' in the '<i>gym.make</i>' line above with the name of any other environment and the rest of the code can stay exactly the same. \n",
    "\n",
    "This is also true for any custom environment that implements the Gym interface. All that's required is a class inherited from the Gym environment and that adds the set of functions described above.\n",
    "\n",
    "This is shown below for the initial framework of the custom 'BabyRobotEnv' that we're going to create (the <i>'_v0'</i> appended to the class name indicates that this is version zero of our environment. We'll update this as we add functionality):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BabyRobotEnv_v0(gym.Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "\n",
    "    def step(self, action):        \n",
    "        state = 1    \n",
    "        reward = -1            \n",
    "        done = True\n",
    "        info = {}\n",
    "        return state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        state = 0\n",
    "        return state\n",
    "  \n",
    "    def render(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this basic framework for our custom environment we've inherited our class from the base 'gym.Env' class, which gives us all of the main functionality required to create the environment. To this we've then added the 4 functions that are required to turn the class into our own, custom, environment:\n",
    "<br/><br/>\n",
    "\n",
    "* '<i>\\_\\_init\\_\\_</i>': the class initialisation, where we can setup anything required by the class.<br/>\n",
    "<br/>\n",
    "* '_step_': implements what happens when Baby Robot takes a step in the environment and returns information describing the results of taking that step.<br/>\n",
    "<br/>\n",
    "* '_reset_': called at the start of every episode to put the environment back into its initial state.<br/>\n",
    "<br/>\n",
    "* '_render_': provides a graphical or text based representation of the environment, to allow the user to see how things are progressing.<br/>\n",
    "<br/>\n",
    "\n",
    "We haven't implemented a '_close_' function, since there's currently nothing to close, so we can just rely on the base class to do any required clean up. Additionally, we haven't yet added any functionality. Our class satisfies the requirements of the Gym interface, and could be used within a Gym test harness, but it currently won't do much!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Action and Observation Spaces\n",
    "\n",
    "The code above defines the framework for a custom environment, however it can't yet be run since it currently has no <i><b>'action_space'</b></i> from which to sample random actions. The <i><b>'action_space'</b></i> defines the set of actions that an agent may take in the environment. These can be discrete, continuous or a combination of both.\n",
    "\n",
    "* <i><b>Discrete actions</b></i> represent a mutually-exclusive set of possible actions, such as the left and right actions in the _CartPole_ environment. At any time-step you can either choose left or right but not both.\n",
    "\n",
    "* <i><b>Continuous actions</b></i> are actions that have an associated value, which represents the amount of that action to take. For example, when turning a steering wheel an angle could be specified to represent by how much the wheel should be turned.\n",
    "\n",
    "The _Baby Robot_ environment that we're creating is what's referred to as a _Grid World_. In other words, it's a grid of squares where _Baby Robot_ may move around, from square to square, to explore and navigate the environment. The default level in this environment will be a 3 x 3 grid, with a starting point at the top left-hand corner, and an exit at the bottom right-hand corner, as shown in Figure 2:\n",
    "\n",
    "\n",
    "<br/>\n",
    "<center><img src=\"./images/default_grid.png\" style=\"background-color: white;\"/></center>\n",
    "<center><i>Figure 2: The default level in the Baby Robot environment.</i></center><br/>\n",
    "\n",
    "\n",
    "Therefore, for the custom _BabyRobotEnv_ that we're creating, there are only 4 possible movement actions: North, South, East or West. Additionally, we'll add a _'Stay'_ action, where Baby Robot remains in the current position. So, in total we have 5 mutually-exclusive actions and we therefore set the action space to define 5 discrete values:\n",
    "<br><br>\n",
    "\n",
    "``` Python\n",
    "self.action_space = gym.spaces.Discrete(5)\n",
    "```\n",
    "\n",
    "<br>\n",
    "In addition to an <i>action_space</i>,  all environments need to specify an <i><b>observation_space</b></i>. This defines the information supplied to the agent when it receives an observation about the environment.\n",
    "\n",
    "When Baby Robot takes a step in the environment we want to return his new position. Therefore we'll define an observation space that specifies a grid position as an '_x_' and '_y_' coordinate.\n",
    "\n",
    "The Gym interface defines a couple of different _['spaces'](https://www.gymlibrary.ml/content/spaces/)_ that could be used to specify our coordinates. For example, if our coordinates where continuous  floating point values we could use the _[Box space](https://www.gymlibrary.ml/content/spaces/#box)_. This would also let us set a limit on the possible range of values that can be used for the 'x' and 'y' coordinates. Additionally, we could then combine these to form a single expression of the environment's observation space using _[Gym's Dict space](https://www.gymlibrary.ml/content/spaces/#dict)_.\n",
    "\n",
    "\n",
    "However, since we're only going to allow whole moves from one square to the next (as opposed to being half-way between squares), we will specify the grid-coordinate in integers. Therefore, as with the action space, we'll be using a discrete set of values. But now, instead of there only being a single discrete value, we have two: one for each of the '_x_' and '_y_' coordinates. Luckily for us, the Gym interface has just the thing, the _[MultiDiscrete space](https://www.gymlibrary.ml/content/spaces/#multidiscrete)_.\n",
    "\n",
    "In the horizontal direction the maximum '_x_' position is bounded by the width of the grid and in the vertical '_y_' direction by the height of the grid. Therefore, the observation space can be defined as follows:\n",
    "<br><br>\n",
    "\n",
    "``` Python\n",
    "\n",
    "self.observation_space = MultiDiscrete([ self.max_x, self.max_y ])\n",
    "\n",
    "```\n",
    "\n",
    "Discrete spaces are zero based, so our coordinate values will be from zero up to one less than the defined maximum value.\n",
    "\n",
    "<br>\n",
    "With these changes the new version of the _BabyRobotEnv_ class is as shown below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym.spaces import Discrete,MultiDiscrete\n",
    "\n",
    "\n",
    "class BabyRobotEnv_v1(gym.Env):\n",
    "  \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # dimensions of the grid\n",
    "        self.width = kwargs.get('width',3)\n",
    "        self.height = kwargs.get('height',3)      \n",
    "      \n",
    "        # define the maximum x and y values\n",
    "        self.max_x = self.width - 1\n",
    "        self.max_y = self.height - 1\n",
    "\n",
    "        # there are 5 possible actions: move N,E,S,W or stay in same state\n",
    "        self.action_space = Discrete(5)          \n",
    "\n",
    "        # the observation will be the coordinates of Baby Robot            \n",
    "        self.observation_space = MultiDiscrete([self.width, self.height])\n",
    "                                        \n",
    "        # Baby Robot's position in the grid\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "\n",
    "    def step(self, action):                \n",
    "        obs = np.array([self.x,self.y])        \n",
    "        reward = -1            \n",
    "        done = True\n",
    "        info = {}\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        # reset Baby Robot's position in the grid\n",
    "        self.x = 0\n",
    "        self.y = 0                \n",
    "        return np.array([self.x,self.y])        \n",
    "  \n",
    "    def render(self,mode):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of points to note about the new version of the _BabyRobotEnv_ class:\n",
    "\n",
    "* We're supplying a <b>_kwargs_</b> argument to the <b>_init_</b> function, letting us create our instance with a dictionary of parameters. Here we're just going to supply the width and height of the grid we want to make, but going forward we can use this to pass other parameters and by using <b>_kwargs_</b> we can avoid changing the interface of the class.\n",
    "\n",
    "* When we take the width and height from the <b>_kwargs_</b>, in both cases we default to values of 3 if the parameter hasn't been supplied. So we'd end up with a grid of size 3x3 if no arguments are supplied during the creation of the environment.\n",
    "\n",
    "* We've now defined Baby Robot's position in the grid using '_self.x_' and '_self.y_', which we now return as the observation from the '_reset_' and '_step_' functions. In both cases we've converted these values into numpy arrays, which although not required to match the Gym interface, is required for the <i>Stable Baseline's</i> environment checker, which will be introduced in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Testing a Custom Environment\n",
    "\n",
    "Before we start adding any real functionality to our custom environment it's worth confirming that our new environment conforms to the Gym interface. To test this we can validate our class using the _[Stable Baselines Environment Checker](https://stable-baselines.readthedocs.io/en/master/common/env_checker.html)_.\n",
    "\n",
    "This will test that we have implemented the functions required to conform to the _Gym_ interface. It also checks that the action and observation spaces are set up correctly and that the function responses match the associated observation space.\n",
    "\n",
    "> One point to note about the environment checker is that, as well as validating that an environment conforms to the _Gym_ standard, it's also checking that the environment is suitable to be run with the _Stable Baseline's_ RL algorithm set. As part of this it expects the observations to be returned as numpy arrays, which is why they've been added in the '_reset_' and '_step_' functions shown above. \n",
    "\n",
    "To run the check it's simply a case of creating an instance of the environment and supplying this to the '_check_env_' function. If there's anything wrong then warning messages will be shown. If there's no output then it's all good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# create an instance of our custom environment\n",
    "env = BabyRobotEnv_v1()\n",
    "\n",
    "# use StableBaselines to check the environment\n",
    "# - returns nothing if the environment is verified as ok\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the environment's action and observation spaces, to make sure they're returning the expected values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(5)\n",
      "Action Space Sample: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Action Space: {env.action_space}\")\n",
    "print(f\"Action Space Sample: {env.action_space.sample()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the action space, as expected, is a Discrete space with 5 possible values.\n",
    "\n",
    "* the value sampled from the action space will be a random value between 0 and 4.\n",
    "\n",
    "Similarly, for the observation space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space: MultiDiscrete([3 3])\n",
      "Observation Space Sample: [1 2]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Observation Space: {env.observation_space}\")\n",
    "print(f\"Observation Space Sample: {env.observation_space.sample()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the observation space has a MultiDiscrete type and its two components each have 3 possible values (since we created a default 3x3 grid).\n",
    "\n",
    "* when sampling from the observation space for this grid, both 'x' and 'y' can take the values 0, 1 or 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Creating the Environment\n",
    "\n",
    "You may have noticed that in the test above, rather than creating the environment using '_gym.make_', as we did for _CartPole_, we instead simply created an instance of it by doing:\n",
    "\n",
    "```Python\n",
    "env = BabyRobotEnv()\n",
    "```\n",
    "\n",
    "This is absolutely fine when working with the environment ourselves, but if we want to have our custom environment registered as a proper _Gym_ environment, that can be created using '_gym.make_', then there are a couple of further steps we need to take.\n",
    "\n",
    "Firstly, from the _[Gym Documentation](https://www.gymlibrary.ml/content/environment_creation/)_, we need to setup our files and directories with a structure similar to that shown below:\n",
    "\n",
    "\n",
    "<br/>\n",
    "<center><img src=\"./images/gym_directory_structure.png\"/></center>\n",
    "<center><i>Figure 2: Directory structure for a custom Gym environment.</i></center><br/>\n",
    "\n",
    "\n",
    "So we need 3 directories:\n",
    "<br><br>\n",
    "\n",
    "__1__\\.&nbsp; The main directory (in this case '_BabyRobotGym_') to hold our '_setup.py_' file. This file defines the name of the project directory and references the required resources, which in this case is just the '_Gym_' library. The contents of this file are as shown below:\n",
    "<br><br>\n",
    "\n",
    "\n",
    "```Python\n",
    "from setuptools import setup\n",
    "\n",
    "setup(name='baby_robot_gym',\n",
    "      version='0.0.1',\n",
    "      install_requires=['gym']  \n",
    ")\n",
    "```\n",
    "<br><br>\n",
    "\n",
    "__2__\\.&nbsp; The project directory, which has the same name as the setup file's '_name_' parameter. So in the case the directory is called '<i>baby_robot_gym</i>'. This contains a single file <i>'\\_\\_init.py\\_\\_'</i> which defines the available versions of the environment:\n",
    "<br><br>\n",
    "\n",
    "\n",
    "```Python\n",
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='BabyRobotEnv-v0',\n",
    "    entry_point='baby_robot_gym.envs:BabyRobotEnv_v0',\n",
    ")\n",
    "\n",
    "register(\n",
    "    id='BabyRobotEnv-v1',\n",
    "    entry_point='baby_robot_gym.envs:BabyRobotEnv_v1',\n",
    ")\n",
    "```\n",
    "<br><br>\n",
    "\n",
    "__3__\\.&nbsp; The '_envs_' directory where the main functionality lives. In our case this contains the two versions of the Baby Robot environment that we've defined above ('<i>baby_robot_env_v0.py</i>' and '<i>baby_robot_env_v1.py</i>'). These define the two classes that are referenced in the '<i>babyrobot/\\_\\_init\\_\\_.py</i>' file.\n",
    "\n",
    "Additionally this directory contains its own '<i>\\_\\_init\\_\\_.py<\\i>' file that references both of the files contained in the directory:\n",
    "<br><br>\n",
    "\n",
    "```Python\n",
    "from .baby_robot_env_v0 import BabyRobotEnv_v0\n",
    "from .baby_robot_env_v1 import BabyRobotEnv_v1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now defined a Python package that can be uploaded to a repository, such as _[PyPi](https://pypi.org/)_, to allow easy sharing of your new creation. Additionally, with this structure in place, we're now able to import our new environment and create it using the _'gym.make'_ method, as we did previously for _CartPole_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import babyrobot\n",
    "\n",
    "# create an instance of our custom environment\n",
    "env = gym.make('BabyRobotEnv-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the name used to specify the environment is the one that was used to register it, not the class name. So, in this case, although the class is called <i>'BabyRobotEnv_v1'</i>, the registered name is actually _'BabyRobotEnv-v1'_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Cloning the Github repository\n",
    "\n",
    "To make it easy to examine the directory structure described above, it can be recreated by cloning the _Github_ repository. The steps to do this are as follows:\n",
    "<br><br>\n",
    "\n",
    "1\\.&nbsp; <b><i>Get the code and move to the newly created directory</b></i>:\n",
    "\n",
    "`git clone https://github.com/WhatIThinkAbout/BabyRobotGym.git` <br>\n",
    "`cd BabyRobotGym`\n",
    "\n",
    "* this directory contains the files and folder structure that we've defined above (plus a few extra ones that we'll look at in part 2).\n",
    "\n",
    "<br><br>\n",
    "2\\.&nbsp; <b><i>Create a Conda environment and install the required packages</b></i>:<br>\n",
    "\n",
    "To be able to run our environment we need to have a few other packages installed, most notably 'Gym' itself. To make it easy to setup the environment the Github repo contains a couple of '.yml' files that list the required packages. \n",
    "To use these to create a Conda environment and install the packages, do the following (choose the one appropriate for your operating system):\n",
    "\n",
    "On Unix:\n",
    "\n",
    "`conda env create -f environment_unix.yml`<br>\n",
    "\n",
    "\n",
    "On Windows: \n",
    "\n",
    "`conda env create -f environment_windows.yml`<br>\n",
    "\n",
    "\n",
    "<br><br>\n",
    "3\\.&nbsp; <b><i>Activate the environment</b></i>:\n",
    "\n",
    "We've created the environment with all our required packages, so now it's just a case of activating it, as follows:\n",
    "\n",
    "`conda activate BabyRobotGym`<br>\n",
    "\n",
    "(when you're finished playing with this environment run \"conda deactivate\" to get back out)\n",
    "\n",
    "\n",
    "<br><br>\n",
    "4\\.&nbsp; <b><i>Run the notebook</b></i>\n",
    "\n",
    "Everything should now be in place to run our custom Gym environment. To test this we can run the sample Jupyter Notebook <i>'baby_robot_gym_test.ipynb'</i> that's included in the repository. This will load the _'BabyRobotEnv-v1'_ environment and test it using the Stable Baseline's environment checker. \n",
    "\n",
    "To start this in a browser, just type:\n",
    "\n",
    "`jupyter notebook baby_robot_gym_test.ipynb`<br>\n",
    "\n",
    "Or else just open this file in VS Code and make sure _'BabyRobotGym'_ is selected as the kernel. This should make the _'BabyRobotEnv-v1'_ environment, test it in Stable Baselines and then run the environment until it completes, which happens to occur in a single step, since we haven't yet written the 'step' function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Adding Actions\n",
    "\n",
    "Although the current version of the custom environment satisfies the requirements of the Gym interface and has the required functions to pass the environment checker tests, it doesn't yet do anything. We want Baby Robot to be able to move around in his environment and for this we're going to need him to be able to take some actions. \n",
    "\n",
    "Since Baby Robot will be operating in a simple Grid World environment (see figure 2, above) the actions he can take will be limited to moving North, South, East or West. Additionally we want him to be able to stay in the same place, if this would be the optimal action. So in total we have 5 possible actions (as we've already seen in the action space).\n",
    "\n",
    "This can be described using a Python integer enumeration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "\n",
    "''' simple helper class to enumerate actions in the grid levels '''\n",
    "class Actions(IntEnum):  \n",
    "    Stay  = 0    \n",
    "    North = 1\n",
    "    East  = 2\n",
    "    South = 3\n",
    "    West  = 4\n",
    "\n",
    "    # get the enum name without the class\n",
    "    def __str__(self): return self.name  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the code we can inherit from our previous '<i>BabyRobotEnv_v1</i>' class. This gives us all of the previous functionality and behaviour, which we can then extend to add the new parts that relate to actions. This is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BabyRobotEnv_v2( BabyRobotEnv_v1 ):\n",
    "\n",
    "  metadata = {'render_modes': ['human']}\n",
    "  \n",
    "  def __init__(self, **kwargs):\n",
    "      super().__init__(**kwargs)\n",
    "      \n",
    "      # the start and end positions in the grid\n",
    "      # - by default these are the top-left and bottom-right respectively\n",
    "      self.start = kwargs.get('start',[0,0])       \n",
    "      self.end = kwargs.get('end',[self.max_x,self.max_y])        \n",
    "      \n",
    "      # Baby Robot's initial position\n",
    "      # - by default this is the grid start \n",
    "      self.initial_pos = kwargs.get('initial_pos',self.start)  \n",
    "\n",
    "      # Baby Robot's position in the grid\n",
    "      self.x = self.initial_pos[0]\n",
    "      self.y = self.initial_pos[1]\n",
    "      \n",
    "      \n",
    "  def take_action(self, action):\n",
    "      ''' apply the supplied action '''\n",
    "      \n",
    "      # move in the direction of the specified action\n",
    "      if   action == Actions.North: self.y -= 1\n",
    "      elif action == Actions.South: self.y += 1\n",
    "      elif action == Actions.West:  self.x -= 1\n",
    "      elif action == Actions.East:  self.x += 1    \n",
    "      \n",
    "      # make sure the move stays on the grid\n",
    "      if self.x < 0: self.x = 0\n",
    "      if self.y < 0: self.y = 0\n",
    "      if self.x > self.max_x: self.x = self.max_x\n",
    "      if self.y > self.max_y: self.y = self.max_y        \n",
    "\n",
    "        \n",
    "  def step(self, action): \n",
    "\n",
    "      # take the action and update the position\n",
    "      self.take_action(action)      \n",
    "      obs = np.array([self.x,self.y])\n",
    "            \n",
    "      # set the 'done' flag if we've reached the exit\n",
    "      done = (self.x == self.end[0]) and (self.y == self.end[1])\n",
    "      \n",
    "      # get -1 reward for each step\n",
    "      # - except at the terminal state which has zero reward\n",
    "      reward = 0 if done else -1\n",
    "        \n",
    "      info = {}\n",
    "      return obs, reward, done, info     \n",
    "\n",
    "\n",
    "  def render(self, mode='human', action=0, reward=0 ):\n",
    "      if mode == 'human':\n",
    "        print(f\"{Actions(action): <5}: ({self.x},{self.y}) reward = {reward}\") \n",
    "      else:\n",
    "        super().render(mode=mode) # just raise an exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new functionality, that's been added to the class, does the following:<br>\n",
    "\n",
    "* in the <i>'\\_\\_init\\_\\_'</i> function key word arguments can be supplied that specify the start and end positions in the environment and Baby Robot's starting position (which by default is set to the grid's start position).\n",
    "\n",
    "* the '<i>take_action</i>' function simply updates Baby Robot's current position by applying the supplied action and then checks that the new position is valid (to stop him going off the grid).\n",
    "\n",
    "* the '_step_' function applies the current action and then gets the new observation and reward, which are then returned to the caller. By default a reward of -1 is returned for each move, unless Baby Robot has reached the end position, in which case the reward is set to zero and the '_done_' flag is set to true.\n",
    "\n",
    "* the '_render_' function prints out the current position and reward.\n",
    "\n",
    "\n",
    "So, finally, we can now take actions and move around from one cell to the next. We can then use a modified version of Listing 1 above (changing from using _CartPole_ to instead use our latest <i>BabyRobot_v2</i> environment) to select random actions and move around the grid until Baby Robot reaches the cell that has been specified as the exit of the grid (which by default is cell (2,2)).\n",
    "\n",
    "The test framework for our new environment is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stay : (0,0) reward = -1\n",
      "Stay : (0,0) reward = -1\n",
      "Stay : (0,0) reward = -1\n",
      "West : (0,0) reward = -1\n",
      "East : (1,0) reward = -1\n",
      "West : (0,0) reward = -1\n",
      "North: (0,0) reward = -1\n",
      "North: (0,0) reward = -1\n",
      "North: (0,0) reward = -1\n",
      "East : (1,0) reward = -1\n",
      "East : (2,0) reward = -1\n",
      "Stay : (2,0) reward = -1\n",
      "Stay : (2,0) reward = -1\n",
      "North: (2,0) reward = -1\n",
      "Stay : (2,0) reward = -1\n",
      "North: (2,0) reward = -1\n",
      "East : (2,0) reward = -1\n",
      "West : (1,0) reward = -1\n",
      "East : (2,0) reward = -1\n",
      "South: (2,1) reward = -1\n",
      "South: (2,2) reward = 0\n"
     ]
    }
   ],
   "source": [
    "# create the cartpole environment\n",
    "env = gym.make('BabyRobotEnv-v2')\n",
    "\n",
    "# initialize the environment\n",
    "env.reset()\n",
    "\n",
    "done = False\n",
    "while not done:  \n",
    "\n",
    "  # choose a random action\n",
    "  action = env.action_space.sample()   \n",
    "\n",
    "  # take the action and get the information from the environment\n",
    "  new_state, reward, done, info = env.step(action)\n",
    "  \n",
    "  # show the current position and reward\n",
    "  env.render(action=action, reward=reward)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path through the grid moves from the start square (0,0) to the exit (2,2). Since actions are chosen at random the path can be of any length. Note also that each step receives a reward of -1, until the exit is reached. So the longer it takes Baby Robot to reach the exit, the more negative the return value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Creating the Render Function\n",
    "\n",
    "Technically we've already created the render function, it's just that it's not very exciting! As can be seen from the output above, all we're getting are simple text messages that describe the action, position and reward. What we really want is a graphical representation of the environment, showing Baby Robot moving around the grid world.\n",
    "\n",
    "As described above, the collection of environments in the Gym library perform their rendering, to show the current state of the environment, either by generating a text based representation or by creating an array containing an image.\n",
    "\n",
    "Text based representations provide a quick way to render the environment in terminal based applications. They're ideal when you only need a simple overview of the current state.\n",
    "\n",
    "Images on the other hand give a very detailed picture of the current state and are perfect for creating videos of an episode, to display after the episode has completed.\n",
    "While both of these representations are useful, neither is particularly suited to creating real-time, detailed, views of the environment's state when working in _Jupyter Notebooks_. When Baby Robot moves around a grid level we want to actually see him moving, rather than just getting a text message describing his position, or watching a simple text drawing, with an '_X_' moving over a grid of dots. \n",
    "\n",
    "Additionally we want to watch this happening as the episode unfolds, rather than only being able to watch it back afterwards, or see it in a flickering display in real-time. In short, we want to render using a different method to text characters or image arrays. We can achieve this by drawing to an HTML5 Canvas, using the excellent _[ipycanvas](https://ipycanvas.readthedocs.io/en/latest/)_ library, and we'll cover this fully in Part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Summary\n",
    "\n",
    "<b><i>OpenAI Gym</b></i> environments are the standard method for testing Reinforcement Learning algorithms. The base collection comes with a large set of varied and challenging problems. However, in many cases you may want to define your own, custom, environment. By implementing the structure and interface of the _Gym_ environment it's easy to create such an environment, that will slot seamlessly into any application that also uses the _Gym_ interface.\n",
    "\n",
    "In summary, the main steps to create a custom Gym environment are as follows:<br>\n",
    "\n",
    "* Create a class that inherits from the env.Gym base class.\n",
    "\n",
    "* Implement the '_reset_', '_step_' and '_render_' functions (and possibly the '_close_' function if resources need to be tidied up).\n",
    "\n",
    "* Define the <b><i>action space</b></i>, to specify the number and type of actions that the environment allows.\n",
    "\n",
    "* Define the <b><i>observation space</b></i>, to describe the information that is supplied to the agent on each step and that sets the boundaries for movement within the environment.\n",
    "\n",
    "* Organise the directory structure and add <i>'\\_\\_init\\_\\_.py'</i> and '_setup.py_' files to match the _Gym_ specification and to make the environment compatible with the _Gym_ framework.\n",
    "\n",
    "Following these steps will give you a bare-bones framework, from which you can start adding your own custom features, to tailor the environment to your own specific problem.\n",
    "\n",
    "In our case, we want to create a Grid World environment that Baby Robot can explore. Additionally, we want to be able to graphically view this environment and watch Baby Robot as he moves around it. In the next we'll see how this can be achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<center><img src=\"images/green_babyrobot_small.gif\"/></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "# References:\n",
    "\n",
    "\n",
    "1. _[The Gym Library](https://www.gymlibrary.ml/)_\n",
    "\n",
    "2. _[Stable Baselines Environment Checker](https://stable-baselines.readthedocs.io/en/master/common/env_checker.html)_\n",
    "\n",
    "3. _[A good YouTube video on custom Gym environments  with Stable Baselines](https://www.youtube.com/watch?v=uKnjGn8fF70)_\n",
    "\n",
    "4. And the complete series of Baby Robot's guide to Reinforcement Learning can be found here … _[Baby Robot Guide](https://towardsdatascience.com/tagged/baby-robot-guide)_"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50f7925c2b527e04ad4ab9285d4738429ed4ef149c3803ef7aee3c43b8d710c9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('BabyRobotGym')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
